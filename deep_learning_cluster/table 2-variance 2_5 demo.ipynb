{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ce6167f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch # For building the networks \n",
    "import torchtuples as tt # Some useful functions\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pycox.datasets import support\n",
    "import numpy as np\n",
    "import cox\n",
    "from cox import CoxPH\n",
    "#from pycox.evaluation import EvalSurv\n",
    "from pycox.preprocessing.feature_transforms import OrderedCategoricalLong\n",
    "import pandas as pd\n",
    "import model\n",
    "from model import Cox_cluster\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn_pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "import seaborn as sns\n",
    "\n",
    "from pycox.evaluation import EvalSurv\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "\n",
    "from pycox.preprocessing.feature_transforms import OrderedCategoricalLong\n",
    "\n",
    "import cox\n",
    "from cox import CoxPH\n",
    "#from pycox.evaluation import EvalSurv\n",
    "from pycox.preprocessing.feature_transforms import OrderedCategoricalLong\n",
    "\n",
    "import model\n",
    "from model import Cox_cluster\n",
    "\n",
    "#np.random.seed(43)\n",
    "#_ = torch.manual_seed(9)\n",
    "file_name_train=os.listdir(r\"/Users/ruiwenzhou/Downloads/var2_5_15 covariates/train\")\n",
    "file_name_test=os.listdir(r\"/Users/ruiwenzhou/Downloads/var2_5_15 covariates/test\")\n",
    "file_name_val=os.listdir(r\"/Users/ruiwenzhou/Downloads/var2_5_15 covariates/val\")\n",
    "\n",
    "file_name_re_train=os.listdir(r\"/Users/ruiwenzhou/Downloads/var2_5_15 covariates/re_train\")\n",
    "file_name_re_test=os.listdir(r\"/Users/ruiwenzhou/Downloads/var2_5_15 covariates/re_test\")\n",
    "file_name_re_val=os.listdir(r\"/Users/ruiwenzhou/Downloads/var2_5_15 covariates/re_val\")\n",
    "\n",
    "#file_name_train=os.listdir(r\"/Users/ruiwenzhou/Downloads/nn kindney simulation cr70 all continous zhong copula/var3_5/train\")\n",
    "#file_name_test=os.listdir(r\"/Users/ruiwenzhou/Downloads/nn kindney simulation cr70 all continous zhong copula/var3_5/test\")\n",
    "#file_name_val=os.listdir(r\"/Users/ruiwenzhou/Downloads/nn kindney simulation cr70 all continous zhong copula/var3_5/val\")\n",
    "\n",
    "#file_name_re_train=os.listdir(r\"/Users/ruiwenzhou/Downloads/nn kindney simulation cr70 all continous zhong copula/var3_5/re_train\")\n",
    "#file_name_re_test=os.listdir(r\"/Users/ruiwenzhou/Downloads/nn kindney simulation cr70 all continous zhong copula/var3_5/re_test\")\n",
    "#file_name_re_val=os.listdir(r\"/Users/ruiwenzhou/Downloads/nn kindney simulation cr70 all continous zhong copula/var3_5/re_val\")\n",
    "#print(file_name_train)\n",
    "def function_work(x):\n",
    "    y = x.rsplit('.', 2)[-2]\n",
    "    return ('log' not in x, int(y) if y.isdigit() else float('inf'), x)\n",
    "\n",
    "csvFiles = file_name_train\n",
    "csvFiles_train=sorted(sorted(csvFiles, key=function_work, reverse=False),key=len, reverse=False)\n",
    "csvFiles = file_name_test\n",
    "csvFiles_test=sorted(sorted(csvFiles, key=function_work, reverse=False),key=len, reverse=False)\n",
    "csvFiles = file_name_val\n",
    "csvFiles_val=sorted(sorted(csvFiles, key=function_work, reverse=False),key=len, reverse=False)\n",
    "csvFiles = file_name_re_train\n",
    "csvFiles_re_train=sorted(sorted(csvFiles, key=function_work, reverse=False),key=len, reverse=False)\n",
    "csvFiles = file_name_re_test\n",
    "csvFiles_re_test=sorted(sorted(csvFiles, key=function_work, reverse=False),key=len, reverse=False)\n",
    "csvFiles = file_name_re_val\n",
    "csvFiles_re_val=sorted(sorted(csvFiles, key=function_work, reverse=False),key=len, reverse=False)\n",
    "arr=[]\n",
    "arr = [0 for i in range(100)] \n",
    "#csvFiles_test\n",
    "arr_ibs=[]\n",
    "arr_ibs = [0 for i in range(100)] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03406286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Initialize variables to track total time\n",
    "total_time = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16bedfcb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'csvFiles_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mcsvFiles_train\u001b[49m)):\n\u001b[1;32m      2\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;66;03m# Start tracking time\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     file_path_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/ruiwenzhou/Downloads/var2_5_15 covariates/train\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mcsvFiles_train[i]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'csvFiles_train' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(len(csvFiles_train)):\n",
    "    start_time = time.time()  # Start tracking time\n",
    "\n",
    "    file_path_train=r\"/Users/ruiwenzhou/Downloads/var2_5_15 covariates/train\"+\"/\"+csvFiles_train[i]\n",
    "    data1_train=pd.read_csv(file_path_train)\n",
    "    file_path_test=r\"/Users/ruiwenzhou/Downloads/var2_5_15 covariates/test\"+\"/\"+csvFiles_test[i]\n",
    "    data1_test=pd.read_csv(file_path_test)\n",
    "    file_path_val=r\"/Users/ruiwenzhou/Downloads/var2_5_15 covariates/val\"+\"/\"+csvFiles_val[i]\n",
    "    data1_val=pd.read_csv(file_path_val)\n",
    "    #print(i)\n",
    "    #print(csvFiles_re_train[i])\n",
    "    file_path_re_train=r\"/Users/ruiwenzhou/Downloads/var2_5_15 covariates/re_train\"+\"/\"+csvFiles_re_train[i]\n",
    "    re_train=pd.read_csv(file_path_re_train)\n",
    "   # print(csvFiles_re_test[i])\n",
    "    file_path_re_test=r\"/Users/ruiwenzhou/Downloads/var2_5_15 covariates/re_test\"+\"/\"+csvFiles_re_test[i]\n",
    "   # print(csvFiles_re_test[i])\n",
    "    re_test=pd.read_csv(file_path_re_test)\n",
    "    file_path_re_val=r\"/Users/ruiwenzhou/Downloads/var2_5_15 covariates/re_val\"+\"/\"+csvFiles_re_val[i]    \n",
    "#    file_path_train=r\"/Users/ruiwenzhou/Downloads/nn kindney simulation cr70 all continous zhong copula/var3_5/train\"+\"/\"+csvFiles_train[i]\n",
    "#    data1_train=pd.read_csv(file_path_train)\n",
    "##    file_path_test=r\"/Users/ruiwenzhou/Downloads/nn kindney simulation cr70 all continous zhong copula/var3_5/test\"+\"/\"+csvFiles_test[i]\n",
    "#    data1_test=pd.read_csv(file_path_test)\n",
    "#    file_path_val=r\"/Users/ruiwenzhou/Downloads/nn kindney simulation cr70 all continous zhong copula/var3_5/val\"+\"/\"+csvFiles_val[i]\n",
    "#    data1_val=pd.read_csv(file_path_val)\n",
    "    #print(i)\n",
    "    #print(csvFiles_re_train[i])\n",
    "#    file_path_re_train=r\"/Users/ruiwenzhou/Downloads/nn kindney simulation cr70 all continous zhong copula/var3_5/re_train\"+\"/\"+csvFiles_re_train[i]\n",
    "#    re_train=pd.read_csv(file_path_re_train)\n",
    "   # print(csvFiles_re_test[i])\n",
    "#    file_path_re_test=r\"/Users/ruiwenzhou/Downloads/nn kindney simulation cr70 all continous zhong copula/var3_5/re_test\"+\"/\"+csvFiles_re_test[i]\n",
    "   # print(csvFiles_re_test[i])\n",
    "#    re_test=pd.read_csv(file_path_re_test)\n",
    "#    file_path_re_val=r\"/Users/ruiwenzhou/Downloads/nn kindney simulation cr70 all continous zhong copula/var3_5/re_val\"+\"/\"+csvFiles_re_val[i]\n",
    "    re_val=pd.read_csv(file_path_re_val)\n",
    "    df_test = data1_test\n",
    "    df_train = data1_train\n",
    "    df_val = data1_val\n",
    "    #cols_std = ['x1', 'x2', 'x3', 'x4','x5','x6','x7']\n",
    "    cols_std = ['x1', 'x2', 'x3', 'x4','x5','x6','x7','x9','x10','x11','x12','x13','x14','x15']\n",
    "    cols_ =  ['x8']\n",
    "    standardize = [([col], StandardScaler()) for col in cols_std]\n",
    "    leave = [(col, None) for col in cols_]\n",
    "\n",
    "\n",
    "    x_mapper = DataFrameMapper(standardize+leave)\n",
    "\n",
    "    x_train = x_mapper.fit_transform(df_train).astype('float32')\n",
    "    x_test = x_mapper.transform(df_test).astype('float32')\n",
    "\n",
    "    re_test = re_test\n",
    "    re_train = re_train\n",
    "\n",
    "    x_train = x_mapper.fit_transform(df_train).astype('float32')\n",
    "#x_val = x_mapper.transform(df_val).astype('float32')\n",
    "    x_test = x_mapper.transform(df_test).astype('float32')\n",
    "    x_val = x_mapper.transform(df_val).astype('float32')\n",
    "#x_train = float('.'.join(str(elem) for elem in x_train[0]))\n",
    "    re_test = re_test\n",
    "    re_train = re_train\n",
    "    re_val=re_val\n",
    "#x_mapper_long = DataFrameMapper(categorical)  # we need a separate mapper to convert data to 'int64'\n",
    "#x_fit_transform = lambda df: tt.tuplefy(df)\n",
    "#x_transform = lambda df: tt.tuplefy(df)\n",
    "#x_fit_transform = lambda df: tt.tuplefy(x_mapper.fit_transform(df))\n",
    "#x_transform = lambda df: tt.tuplefy(x_mapper.transform(df))\n",
    "    cols_bin_re=list(re_train.columns)[1:]\n",
    "    leave_re = [(col, None) for col in cols_bin_re]\n",
    "#categorical = [(col, OrderedCategoricalLong()) for col in cols_cat]\n",
    "    #print(leave_re)\n",
    "\n",
    "\n",
    "    x_mapper_re= DataFrameMapper(leave_re)\n",
    "    re_train = x_mapper_re.fit_transform(re_train).astype('float32')\n",
    "    #x_val = x_mapper.transform(df_val).astype('float32')\n",
    "    re_test = x_mapper_re.transform(re_test).astype('float32')\n",
    "    re_val = x_mapper_re.transform(re_val).astype('float32')\n",
    "#print(re_train[0].shape)\n",
    "    get_target = lambda df: (df['o'].values, df['delta'].values)\n",
    "    y_train = get_target(df_train)\n",
    "    y_val = get_target(df_val)\n",
    "    durations_test, events_test = get_target(df_test)\n",
    "    val = x_val,re_val, y_val\n",
    "    #print(durations_test)\n",
    "#print(x_train[0].dtype)\n",
    "#print(y_train[0].dtype)\n",
    "#x_train=x_train.drop(columns=[\"split_train\"])\n",
    "    optimizer = tt.optim.AdamWR(decoupled_weight_decay=0.001, cycle_eta_multiplier=0.5,\n",
    "                            cycle_multiplier=2)\n",
    "    In_Nodes= x_train.shape[1]\n",
    "    num_nodes = [64, 64]\n",
    "    Pathway_Nodes=num_nodes[0]\n",
    "    Hidden_Nodes=num_nodes[1]\n",
    "    Out_Nodes= 1\n",
    "    batch_norm = True\n",
    "    dropout = 0.2\n",
    "    output_bias = False\n",
    "    batch_size =128\n",
    "\n",
    "#net = tt.practical.MLPVanilla(in_features, num_nodes, out_features, batch_norm,\n",
    "#                              dropout, output_bias=output_bias)\n",
    "    net= Cox_cluster(In_Nodes, Pathway_Nodes, Hidden_Nodes, Out_Nodes,)\n",
    "    model = cox.CoxPH(net, optimizer)\n",
    "#print(model)\n",
    "\n",
    "\n",
    "#model_2a.optimizer.set_lr(0.06)\n",
    "    epochs = 100\n",
    "    callbacks = [tt.callbacks.EarlyStopping()]\n",
    "    verbose = True\n",
    "\n",
    "#log = model.fit(x_train, re_train,y_train, batch_size, epochs, callbacks, verbose)\n",
    "    batch_size =128\n",
    "    lrfind = model.lr_finder(x_train, re_train,y_train,batch_size, tolerance=100)\n",
    "#_ = lrfind.plot()\n",
    "    print(lrfind.get_best_lr())\n",
    "#0.035111917342151515\n",
    "\n",
    "    model.optimizer.set_lr(lrfind.get_best_lr())\n",
    "    log = model.fit(x_train, re_train,y_train, batch_size, epochs, callbacks, verbose, val_data=val,\n",
    "                val_batch_size=batch_size)\n",
    "#_ = log.plot()\n",
    "#print(model.partial_log_likelihood(*val).mean())\n",
    "#print(np.squeeze(model.partial_log_likelihood(*val)[0],axis=0).shape)\n",
    "#print(np.squeeze(model.partial_log_likelihood(*val)[0],axis=0)+model.partial_log_likelihood(*val)[1])\n",
    "    baseline = model.compute_baseline_hazards(batch_size=100000)\n",
    "   # print(baseline.shape)\n",
    "    surv = model.predict_surv_df(x_test,re_test,baseline_hazards_=baseline)\n",
    "#surv = model.predict_surv_df(x_test,re_test)\n",
    "#print(surv)\n",
    "#print(surv.index.values)\n",
    "#ev = EvalSurv(surv, durations_test, events_test, censor_surv='km')\n",
    "#C_INDEX=ev.concordance_td('antolini')\n",
    "#print(C_INDEX)\n",
    "\n",
    "#time_grid = np.linspace(durations_test.min(), durations_test.max(), 100)\n",
    "#ev.brier_score(time_grid)\n",
    "#surv.flags\n",
    "#np.isfortran(surv)\n",
    "#surv = np.array(surv, order='C')\n",
    "#print(durations_test.shape[0])\n",
    "#print( surv.shape[1] )\n",
    "#print(surv.index.values.shape)\n",
    "#print(events_test.shape[0])\n",
    "    ev = EvalSurv(surv, durations_test, events_test, censor_surv='km')\n",
    "    arr[i]=ev.concordance_td()\n",
    "    print(ev.concordance_td())\n",
    "    time_grid = np.linspace(durations_test.min(), durations_test.max(), 100)\n",
    "    arr_ibs[i]=ev.integrated_brier_score(time_grid)\n",
    "    print(arr_ibs[i])\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time  # Calculate time taken for this round\n",
    "    total_time += elapsed_time  # Accumulate the total time\n",
    "    \n",
    "    print(f\"Round {i+1} took {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "# Calculate the average time per round\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "facb4f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7443845265552489\n",
      "0.19566051827273617\n",
      "0.04860305287206068\n",
      "0.007016158198253037\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "def Average(lst):\n",
    "    return sum(lst) / len(lst)\n",
    "  \n",
    "\n",
    "average_c_index = Average(arr)\n",
    "print(average_c_index)\n",
    "average_ibs = Average(arr_ibs)\n",
    "print(average_ibs)\n",
    "sd_c_index = statistics.stdev(arr)\n",
    "print(sd_c_index)\n",
    "sd_ibs = statistics.stdev(arr_ibs)\n",
    "print(sd_ibs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cd5579e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1686379611.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [52]\u001b[0;36m\u001b[0m\n\u001b[0;31m    0.766196561612089 32\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "0.7466336817715338 32\n",
    "0.7541379274780994 64"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
